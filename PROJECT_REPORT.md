# Project Report: Cutout Detection Analysis

## 1. How to Run (CLI Instructions) üöÄ

The pipeline is designed to be run via a single entry point that handles Denoising ‚Üí Transcription ‚Üí Diarization ‚Üí Analysis automatically.

### üî∑ Primary Command
To process a single audio file (supports mp3, m4a, wav, etc.):
```bash
python process_audio.py --audio "path/to/your/audio.mp3"
```
*   **What it does:**
    1.  Converts audio to 16kHz WAV.
    2.  **Denoises** the audio (removes background noise).
    3.  Runs **Whisper** transcription (Pass 1 & Pass 2).
    4.  Runs **Speaker Diarization** (Auto-detects speaker count).
    5.  Merges results and detects **Cutouts**.
    6.  Saves everything to `outputs/<audio_filename>/`.

### ‚öôÔ∏è Optional Arguments
| Argument | Description | Default | Example |
| :--- | :--- | :--- | :--- |
| `--model` | Whisper model size (tiny, base, small, medium, large) | `large` | `--model medium` |

### üõ†Ô∏è Configuration (`config.yaml`)
Key processing thresholds can be tweaked in the `config.yaml` file without changing code:
```yaml
processing:
  analyzer:
    min_silence_len_ms: 1000      # Silence duration to be considered "silence"
    silence_threshold_db: -40     # Decibels considered silence
  diarization:
    min_segment_duration_s: 0.3   # Ignore speakers shorter than this
    noise_energy_threshold_db: -50 # Ignore quiet noises
  transcription:
    hallucination_silence_threshold: 0.6  # Filter Whisper hallucinations
```

---

## 2. Tools & Technologies Stack üõ†Ô∏è

| Category | Tool / Library | Purpose & Why We Use It |
| :--- | :--- | :--- |
| **Audio Core** | **FFmpeg + Pydub** | **Standardization**: Converts all inputs (mp3, m4a, etc.) to 16kHz mono WAV, the required format for AI models. |
| **Denoising** | **Demucs (Facebook)** | **Vocal Isolation**: Separates vocals from background noise (music, street sounds) before processing. This dramatically improves diarization accuracy. |
| **Transcription** | **OpenAI Whisper** | **Accuracy**: State-of-the-art ASR model. We use a **Two-Pass Strategy**: <br> 1. Translate to English (for content analysis) <br> 2. Transcribe Original Language (for timestamps). |
| **Diarization** | **PyAnnote Audio** | **Speaker ID**: Identifies "who spoke when". We map these segments to "User" or "Agent" based on conversation dynamics (Agent usually starts/ends). |
| **Analysis** | **NumPy & Pandas** | **Math**: Efficiently handles audio arrays and calculates statistical baselines (median, p95) for adaptive thresholds. |
| **Visualization** | **Matplotlib** | **QC**: Generates waveform plots to visually verify audio quality and segmentation. |

## 3. Merging Logic: Combining Whisper & Diarization üîÑ

We combine the text from Whisper with the speaker info from PyAnnote using a **Timestamp Overlap Algorithm**.

### Algorithm Steps:
1.  **Iterate**: Loop through every text segment generated by Whisper.
2.  **Overlap Calculation**: Compare the text segment's start/end times with all Speaker Diarization segments.
3.  **Ratio Check**: Calculate how much of the text segment overlaps with each speaker.
    *   *Formula*: `Overlap Ratio = Overlap Duration / Text Segment Duration`
4.  **Best Match**: Assign the speaker with the highest overlap ratio (minimum 30% required).
5.  **Fallback (Smart Recovery)**:
    *   If no overlap is found (e.g., silence or noise), find the **nearest speaker** within 2.0 seconds.
    *   This ensures that short utterances between speaker turns aren't "lost" or marked as Unknown.

---

## 4. Cutout Detection: How It Works üîç

The detector uses a set of **Contextual Rules** to distinguish between natural pauses and technical cutouts. It doesn't just look for "silence" (because a pause for thinking is normal).

### The Rules of Detection

| Rule | Name | Condition / Logic |
| :--- | :--- | :--- |
| **1** | **Incomplete Speech** | Did a sentence end abruptly without punctuation? <br> *Checks for words like "and", "but", "so" or lack of punctuation.* |
| **2** | **Missing Response** | Did the User ask a question, and Agent failed to respond? <br> *Checks for question marks or question words followed by silence.* |
| **3** | **Mid-Speech Gap** | Did the same speaker pause for > 2s in the middle of a phrase? |
| **4** | **Within-Sentence Dropout** | Tiny gaps (0.7s - 2s) inside a continuous speech segment. <br> *Usually indicates packet loss or technical glitch.* |
| **5** | **User Drop-off** | **Did the user speak for < 1.0s and then vanish?** <br> *Indicates potential disconnection or microphone failure immediately after starting to speak.* |
| **6** | **Agent Latency** | Gaps > 3s before Agent response (not strictly a cutout, but flagged). |

---

### üßÆ Detailed Calculations

Here is the exact math used for the thresholds, including the "User Drop-off" rule you asked about.

#### 1. User Drop-off Calculation
*   **Target**: Trigger when a user starts speaking but cuts out almost immediately.
*   **Formula**:
    $$ Cutout = (D_{speech} < 1.0s) \land (D_{gap} > 5.0s) $$
    *   **$D_{speech}$ (Speech Duration)**: The duration of the user's audio segment before the silence.
        *   *Old Value*: 0.5s
        *   **New Value**: **1.0s** (Updated to be more sensitive)
    *   **$D_{gap}$ (Silence Duration)**: The duration of the silence immediately following the speech.
    *   **Logic**: If a user says something very short (less than 1 second) and then silence follows for more than 5 seconds, it is flagged as a "User Drop-off".

#### 2. Adaptive Thresholds (Smart Detection)
For other rules, we calculate a baseline from the conversation itself to adapt to the speaker's pace.
*   **Median Response Time ($M$)**: The median time the Agent usually takes to respond.
*   **P95 Response Time ($P_{95}$)**: The 95th percentile (slowest 5%) of responses.

**Missing Response Threshold**:
$$ T_{missing} = \max(M \times 2.5, \ P_{95} \times 1.5, \ 3.0s) $$
*   We flag a missing response only if the silence is **2.5x longer** than the median response time.

**Incomplete Speech Threshold**:
$$ T_{incomplete} = \max(M_{pause} \times 1.5, \ P_{90\_pause} \times 1.2, \ 0.8s) $$
*   We flag a pause after incomplete syntax (e.g., "I went to...") if it is **1.5x longer** than their normal pause duration.
